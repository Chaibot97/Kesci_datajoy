eval_metric = "rmse",
maximize = FALSE,
eta = 0.03,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 256,
gamma = 2,
max_depth = 6
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 512,
watchlist = list(
develope = mt_dev,
validate = mt_val
),
early_stopping_rounds = 30,
verbose = 1,
print_every_n = 15
)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.03,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 256,
# gamma = 2,
max_depth = 6
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 512,
watchlist = list(
develope = mt_dev,
validate = mt_val
),
early_stopping_rounds = 30,
verbose = 1,
print_every_n = 15
)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.02,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 256,
gamma = 1,
max_depth = 6
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
develope = mt_dev,
validate = mt_val
),
early_stopping_rounds = 30,
verbose = 1,
print_every_n = 15
)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.02,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 256,
gamma = 1,
max_depth = 5
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
develope = mt_dev,
validate = mt_val
),
early_stopping_rounds = 30,
verbose = 1,
print_every_n = 15
)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.02,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 200,
gamma = 1,
max_depth = 5
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
develope = mt_dev,
validate = mt_val
),
early_stopping_rounds = 30,
verbose = 1,
print_every_n = 15
)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.02,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 200,
gamma = 0.5,
max_depth = 5
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
develope = mt_dev,
validate = mt_val
),
early_stopping_rounds = 30,
verbose = 1,
print_every_n = 15
)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.02,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 200,
gamma = 0.25,
max_depth = 5
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
develope = mt_dev,
validate = mt_val
),
early_stopping_rounds = 30,
verbose = 1,
print_every_n = 15
)
# Create xgb.DMatrix
my.DMatrix <- function(dt, feat_num, feat_cat = NULL) {
if (!is.null(feat_cat)) {
one_hot <- lapply(dt[, feat_cat, with = FALSE], contrasts, contrasts = FALSE)
mt_cat <- model.matrix(
~ . + 0, data = dt[, feat_cat, with = FALSE], contrasts.arg = one_hot
)
mt <- cbind(as.matrix(dt[, feat_num, with = FALSE]), mt_cat)
} else {
mt <- as.matrix(dt[, feat_num, with = FALSE])
}
mt <- xgb.DMatrix(
data = mt,
label = dt[[meta$target]]
)
return(mt)
}
mt_dev <- my.DMatrix(dt_dev, meta$feat_num, meta$feat_cat)
mt_val <- my.DMatrix(dt_val, meta$feat_num, meta$feat_cat)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.03,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 200,
gamma = 0.25,
max_depth = 5
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
develope = mt_dev,
validate = mt_val
),
early_stopping_rounds = 30,
verbose = 1,
print_every_n = 15
)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.03,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 200,
gamma = 0.2,
max_depth = 5
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
develope = mt_dev,
validate = mt_val
),
early_stopping_rounds = 30,
verbose = 1,
print_every_n = 15
)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.03,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 200,
gamma = 0.2,
max_depth = 5
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
develope = mt_dev,
validate = mt_val
),
early_stopping_rounds = 50,
verbose = 1,
print_every_n = 15
)
dt_dev[, pred_quick := predict(model_xgb_quick, mt_dev, ntreelimit = model_xgb_quick$best_ntreelimit)]
dt_val[, pred_quick := predict(model_xgb_quick, mt_val, ntreelimit = model_xgb_quick$best_ntreelimit)]
dt_dev[, pred_quick := predict(model_xgb_quick, mt_dev, ntreelimit = model_xgb_quick$best_ntreelimit)]
dt_val[, pred_quick := predict(model_xgb_quick, mt_val, ntreelimit = model_xgb_quick$best_ntreelimit)]
my.mse <- function(y, y_hat) {
mean((y - y_hat)^2)
}
dt_dev[, pred_quick := predict(model_xgb_quick, mt_dev, ntreelimit = model_xgb_quick$best_ntreelimit)]
dt_val[, pred_quick := predict(model_xgb_quick, mt_val, ntreelimit = model_xgb_quick$best_ntreelimit)]
my.mse <- function(y, y_hat) {
mean((y - y_hat)^2)
}
my.mse(dt_dev$satisfaction_level, dt_dev$pred_quick)
my.mse(dt_val$satisfaction_level, dt_val$pred_quick)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.03,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 200,
gamma = 0.2,
max_depth = 5
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 2048,
watchlist = list(
develope = mt_dev,
validate = mt_val
),
early_stopping_rounds = 50,
verbose = 1,
print_every_n = 15
)
dt_dev[, pred_quick := predict(model_xgb_quick, mt_dev, ntreelimit = model_xgb_quick$best_ntreelimit)]
dt_val[, pred_quick := predict(model_xgb_quick, mt_val, ntreelimit = model_xgb_quick$best_ntreelimit)]
my.mse <- function(y, y_hat) {
mean((y - y_hat)^2)
}
my.mse(dt_dev$satisfaction_level, dt_dev$pred_quick)
my.mse(dt_val$satisfaction_level, dt_val$pred_quick)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.03,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 200,
gamma = 0.2,
max_depth = 6
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
develope = mt_dev,
validate = mt_val
),
early_stopping_rounds = 50,
verbose = 1,
print_every_n = 15
)
dt_dev[, pred_quick := predict(model_xgb_quick, mt_dev, ntreelimit = model_xgb_quick$best_ntreelimit)]
dt_val[, pred_quick := predict(model_xgb_quick, mt_val, ntreelimit = model_xgb_quick$best_ntreelimit)]
my.mse <- function(y, y_hat) {
mean((y - y_hat)^2)
}
my.mse(dt_dev$satisfaction_level, dt_dev$pred_quick)
my.mse(dt_val$satisfaction_level, dt_val$pred_quick)
dt_lst <- list(
obj = dt_full$test,
all = dt_full$train,
dev = dt_full$train[idx_dev],
val = dt_full$train[idx_val]
)
mt_lst <- lapply(
dt_lst,
my.DMatrix,
feat_num = meta$feat_num
)
mt_lst <- lapply(
dt_lst,
my.DMatrix,
feat_num = meta$feat_num
)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.03,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 200,
gamma = 0.2,
max_depth = 6
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
full_set = mt_lst$all,
develope = mt_lst$dev,
validate = mt_lst$val
),
early_stopping_rounds = 50,
verbose = 1,
print_every_n = 15
)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.05,
subsample = 0.8,
colsample_bytree = 0.9,
min_child_weight = 200,
gamma = 0.2,
max_depth = 6
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
full_set = mt_lst$all,
develope = mt_lst$dev,
validate = mt_lst$val
),
early_stopping_rounds = 50,
verbose = 1,
print_every_n = 15
)
# Create xgb.DMatrix
my.DMatrix <- function(dt, feat_num, feat_cat = NULL) {
if (!is.null(feat_cat)) {
one_hot <- lapply(dt[, feat_cat, with = FALSE], contrasts, contrasts = FALSE)
mt_cat <- model.matrix(
~ . + 0, data = dt[, feat_cat, with = FALSE], contrasts.arg = one_hot
)
mt <- cbind(as.matrix(dt[, feat_num, with = FALSE]), mt_cat)
} else {
mt <- as.matrix(dt[, feat_num, with = FALSE])
}
mt <- xgb.DMatrix(
data = mt,
label = dt[[meta$target]]
)
return(mt)
}
mt_lst <- lapply(
dt_lst,
my.DMatrix,
feat_num = meta$feat_num,
feat_cat = meta$feat_cat
)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.05,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 200,
gamma = 0.2,
max_depth = 6
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
full_set = mt_lst$all,
develope = mt_lst$dev,
validate = mt_lst$val
),
early_stopping_rounds = 50,
verbose = 1,
print_every_n = 15
)
args_xgb_quick <- list(
objective = "reg:squarederror",
eval_metric = "rmse",
maximize = FALSE,
eta = 0.03,
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 200,
gamma = 0.15,
max_depth = 6
)
model_xgb_quick <- xgb.train(
params = args_xgb_quick,
data = mt_dev,
nrounds = 1024,
watchlist = list(
full_set = mt_lst$all,
develope = mt_lst$dev,
validate = mt_lst$val
),
early_stopping_rounds = 50,
verbose = 1,
print_every_n = 15
)
. <- sapply(
names(dt_lst),
function(set, dt_lst, mt_lst, model, score_name) {
dt_lst[[set]][, (score_name) := predict(model, mt_lst[[set]], ntreelimit = model$best_ntreelimit)]
},
dt_lst = dt_lst,
mt_lst = mt_lst,
score_name = "pred_quick"
)
. <- sapply(
names(dt_lst),
function(set, dt_lst, mt_lst, model, score_name) {
dt_lst[[set]][, (score_name) := predict(model, mt_lst[[set]], ntreelimit = model$best_ntreelimit)]
},
model = model_xgb_quick,
dt_lst = dt_lst,
mt_lst = mt_lst,
score_name = "pred_quick"
)
my.mse(dt_lst$all$satisfaction_level, dt_lst$all$pred_quick)
my.mse(dt_lst$all$satisfaction_level, dt_lst$all$pred_quick)
. <- sapply(
names(dt_lst),
function(set, dt_lst, mt_lst, model, score_name) {
dt_lst[[set]][, (score_name) := predict(model, mt_lst[[set]], ntreelimit = model$best_ntreelimit)]
},
model = model_xgb_quick,
dt_lst = dt_lst,
mt_lst = mt_lst,
score_name = "pred_quick"
)
my.mse <- function(y, y_hat) {
mean((y - y_hat)^2)
}
my.mse(dt_lst$all$satisfaction_level, dt_lst$all$pred_quick)
my.mse(dt_lst$dev$satisfaction_level, dt_lst$dev$pred_quick)
my.mse(dt_lst$val$satisfaction_level, dt_lst$val$pred_quick)
table(dt_lst$val$pred_quick)
table(dt_lst$val$division)
table(dt_lst$dev$division)
sub <- dt_lst$obj[, .(id, satisfaction_level = pred_quick)]
View(sub)
fwrite(sub, "sub_quick.csv")
fwrite(dt_lst$dev, "data/good_train.csv")
fwrite(dt_lst$val, "data/bad_train.csv")
table(dt_lst$all$salary, dt_lst$all$division)
dt_lst$all[, mean(salary), by = "division"]
library(xgboost)
library(ggplot2)
library(data.table)
library(magrittr)
library(glmnet)
install.packages(c("data.table", "ggplot2", "glmnet", "magrittr", "xgboost"))
.Library
.libPaths()
.libPaths()
